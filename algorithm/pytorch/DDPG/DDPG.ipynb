{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install swig\n",
    "# !pip install gym[all]\n",
    "\n",
    "# !pip install pyvirtualdisplay\n",
    "# !pip install tqdm\n",
    "# !pip install neptune-client\n",
    "# !conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import os\n",
    "# # sys.path.append('set PATH=C:/Users/c3296143/.mujoco/mujoco200/bin;%PATH%')\n",
    "# # sys.path.append('set PATH=C://Users//c3296143//.mujoco//mujoco200//bin;%PATH%')\n",
    "# # sys.path.append('C:/Users/c3296143/.mujoco/mujoco200/bin')\n",
    "# # sys.path.append('C://Users//c3296143//.mujoco//mujoco200//bin')\n",
    "# # os.environ['LD_LIBRARY_PATH']=os.environ['LD_LIBRARY_PATH'] + 'C:/Users/c3296143/root/.mujoco/mujoco200/bin'\n",
    "# # old = os.environ.get(\"LD_LIBRARY_PATH\")\n",
    "# old = os.environ.get(\"PATH\")\n",
    "# # os.environ[\"PATH\"] = 'C:\\> set PATH=%PATH%;C:/Users/c3296143/.mujoco/mujoco200/bin'\n",
    "# # os.environ[\"PATH\"] = 'set PATH=C:/Users/c3296143/.mujoco/mujoco200/bin;%PATH%'\n",
    "\n",
    "# if old:\n",
    "#     os.environ[\"PATH\"] = old + \";\" +'C:\\\\Users\\\\c3296143\\\\.mujoco\\\\mujoco200\\\\bin'\n",
    "# #     os.environ[\"LD_LIBRARY_PATH\"] = old + \":\" + 'C:/Users/c3296143/.mujoco/mujoco200/bin'\n",
    "# # else:\n",
    "# #     os.environ[\"LD_LIBRARY_PATH\"] = 'C:/Users/c3296143/.mujoco/mujoco200/bin'\n",
    "# else:\n",
    "#     os.environ[\"PATH\"] = 'C:\\\\Users\\\\c3296143\\\\.mujoco\\\\mujoco200\\\\bin'\n",
    "\n",
    "# print(os.environ[\"PATH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glfw\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from collections import namedtuple, deque\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import neptune.new as neptune\n",
    "from PIL import Image\n",
    "import imageio\n",
    "from pyvirtualdisplay import Display\n",
    "# Display().start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nep_log  = neptune.init(\n",
    "    project=\"xhnfirst/RA-DDPG-IP-test\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI1NTg5MDI2OS01MTVmLTQ2YjUtODA1Yy02ZWQyNDgxZDcwN2UifQ==\",\n",
    ")  # your credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('InvertedPendulum-v5-down')\n",
    "frame = []\n",
    "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device = ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(sizes, activation, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class MLPActor(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation, act_limit):\n",
    "        super().__init__()\n",
    "        pi_sizes = [obs_dim] + list(hidden_sizes) + [act_dim]\n",
    "        self.pi = mlp(pi_sizes, activation, nn.Tanh)\n",
    "        self.act_limit = act_limit\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Return output from network scaled to action space limits.\n",
    "        return self.act_limit * self.pi(obs)\n",
    "\n",
    "class MLPQFunction(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.q = mlp([obs_dim + act_dim] + list(hidden_sizes) + [1], activation)\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        q = self.q(torch.cat([obs, act], dim=-1))\n",
    "        return torch.squeeze(q, -1) # Critical to ensure q has right shape.\n",
    "\n",
    "class MLPActorCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_sizes=(256,256),\n",
    "                 activation=nn.ReLU, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "        super().__init__()\n",
    "\n",
    "        obs_dim = 4\n",
    "        act_dim = 1\n",
    "        act_limit = 1\n",
    "\n",
    "        # build policy and value functions\n",
    "        self.pi = MLPActor(obs_dim, act_dim, hidden_sizes, activation, act_limit).to(device)\n",
    "        self.q = MLPQFunction(obs_dim, act_dim, hidden_sizes, activation).to(device)\n",
    "\n",
    "    def act(self, obs):\n",
    "        with torch.no_grad():\n",
    "            return self.pi(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('obs', 'act', 'rew', 'next_obs', 'done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"dropout\": 0.2,\n",
    "    \"learning_rate\": 0.001,\n",
    "    # \"optimizer\": \"SGD\",\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"hid\": 64,\n",
    "    \"l\": 3,\n",
    "    \"seed\": 0,\n",
    "    \"steps_per_epoch\": 500,\n",
    "    \"steps_video\": 5000,\n",
    "    \"epochs\": 1000,\n",
    "    \"replay_size\": int(1e8),\n",
    "    \"gamma\": 0.98,\n",
    "    \"polyak\": 0.995,\n",
    "    \"pi_lr\": 1e-4,\n",
    "    \"q_lr\": 1e-4,\n",
    "    \"batch_size\": 1000,\n",
    "    \"start_steps\": 3000, \n",
    "    \"update_after\": 1500,\n",
    "    \"update_every\": 300,\n",
    "    \"act_noise\": 0.01,\n",
    "    \"num_test_episodes\": 5,\n",
    "    \"max_ep_len\": 100,\n",
    "    \"max_video_len\": 100,\n",
    "    \"save_model_len\": 20000,\n",
    "    \"obs_dim\": 4,\n",
    "    \"act_dim\": 1,\n",
    "    \"act_limit\": 1\n",
    "}\n",
    "\n",
    "ac_kwargs=dict(hidden_sizes=[params[\"hid\"]]*params[\"l\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nep_log[\"parameters\"] = params\n",
    "\n",
    "torch.manual_seed(params[\"seed\"])\n",
    "np.random.seed(params[\"seed\"])\n",
    "\n",
    "# Action limit for clamping: critically, assumes all dimensions share the same bound!\n",
    "print('obs_dim = ', params[\"obs_dim\"] )\n",
    "print('act_dim = ', params[\"act_dim\"])\n",
    "print('act_limit = ', params[\"act_limit\"])\n",
    "\n",
    "# Create actor-critic module and target networks\n",
    "ac = MLPActorCritic(**ac_kwargs)\n",
    "ac_targ = deepcopy(ac)\n",
    "\n",
    "# Freeze target networks with respect to optimizers (only update via polyak averaging)\n",
    "for p in ac_targ.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "memory = ReplayMemory(params[\"replay_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up function for computing DDPG Q-loss\n",
    "def compute_loss_q(data):\n",
    "\n",
    "    o = torch.cat(data.obs).float()\n",
    "    a = torch.cat(data.act).float()\n",
    "    r = torch.cat(data.rew).float()\n",
    "    o2 =torch.cat(data.next_obs).float()\n",
    "    d = torch.cat(data.done).float()\n",
    "    q = ac.q(o,a)\n",
    "\n",
    "\n",
    "    # Bellman backup for Q function\n",
    "    with torch.no_grad():\n",
    "        q_pi_targ = ac_targ.q(o2, ac_targ.pi(o2))\n",
    "        backup = r + params[\"gamma\"] * (1 - d) * q_pi_targ\n",
    "\n",
    "    # MSE loss against Bellman backup\n",
    "    loss_q = ((q - backup)**2).mean()\n",
    "\n",
    "    return loss_q\n",
    "\n",
    "# Set up function for computing DDPG pi loss\n",
    "def compute_loss_pi(data):\n",
    "\n",
    "    o = torch.cat(data.obs).float()\n",
    "\n",
    "    q_pi = ac.q(o, ac.pi(o))\n",
    "\n",
    "    return -q_pi.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_optimizer = RMSprop(ac.pi.parameters(), lr=params[\"pi_lr\"])\n",
    "q_optimizer = RMSprop(ac.q.parameters(), lr=params[\"q_lr\"])\n",
    "\n",
    "def update(data):\n",
    "    # First run one gradient descent step for Q.\n",
    "\n",
    "\n",
    "    q_optimizer.zero_grad()\n",
    "    loss_q = compute_loss_q(data)\n",
    "\n",
    "    loss_q.backward()\n",
    "\n",
    "    q_optimizer.step()\n",
    "\n",
    "\n",
    "    # Freeze Q-network so you don't waste computational effort \n",
    "    # computing gradients for it during the policy learning step.\n",
    "    for p in ac.q.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # Next run one gradient descent step for pi.\n",
    "    pi_optimizer.zero_grad()\n",
    "    loss_pi = compute_loss_pi(data)\n",
    "    loss_pi.backward()\n",
    "    pi_optimizer.step()\n",
    "\n",
    "    # Unfreeze Q-network so you can optimize it at next DDPG step.\n",
    "    for p in ac.q.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "    # Finally, update target networks by polyak averaging.\n",
    "    with torch.no_grad():\n",
    "        for p, p_targ in zip(ac.parameters(), ac_targ.parameters()):\n",
    "            # NB: We use an in-place operations \"mul_\", \"add_\" to update target\n",
    "            # params, as opposed to \"mul\" and \"add\", which would make new tensors.\n",
    "            p_targ.data.mul_(params[\"polyak\"])\n",
    "            p_targ.data.add_((1 - params[\"polyak\"]) * p.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(o, noise_scale):\n",
    "    a = ac.act(torch.as_tensor(o, dtype=torch.float32))\n",
    "    # print('a = ', a)\n",
    "    a += noise_scale * torch.randn(params[\"act_dim\"]).to(device)\n",
    "    return torch.clip(a, -params[\"act_limit\"], params[\"act_limit\"])\n",
    "\n",
    "def test_agent(epoch):\n",
    "    test_main = 0\n",
    "    test_step = 0\n",
    "    for j in range(params[\"num_test_episodes\"]):\n",
    "        obs, d, test_ep_ret, test_ep_len = env.reset(), False, 0, 0\n",
    "        o = obs\n",
    "        o = torch.tensor([o], dtype=torch.float32, device=device)\n",
    "        while not(test_ep_len == params[\"max_ep_len\"]):\n",
    "            a_cpu = get_action(o, 0).cpu().data.numpy()\n",
    "            obs, r, d, _ = env.step(a_cpu[0])\n",
    "            o = obs\n",
    "            o = torch.tensor([o], dtype=torch.float32, device=device)\n",
    "\n",
    "            test_ep_ret += r\n",
    "            test_ep_len += 1\n",
    "        test_ep_main = test_ep_ret/test_ep_len\n",
    "        test_step +=1\n",
    "        test_main += test_ep_main\n",
    "\n",
    "    print('test_rew_main = ', float(test_main/test_step))\n",
    "    nep_log[\"test/reward\"].log(test_main/test_step)\n",
    "\n",
    "\n",
    "\n",
    "def video_agent(epoch):\n",
    "    screen = env.render(mode='rgb_array')\n",
    "    im = Image.fromarray(screen)\n",
    "    images = [im]\n",
    "    obs, d, test_ep_ret, test_ep_len = env.reset(), False, 0, 0\n",
    "    o = obs\n",
    "    o = torch.tensor([o], dtype=torch.float32, device=device)\n",
    "    while not(test_ep_len == params[\"max_video_len\"]):\n",
    "        a_cpu = get_action(o, 0).cpu().data.numpy()\n",
    "        obs, r, d, _ = env.step(a_cpu[0])\n",
    "        screen = env.render(mode='rgb_array')\n",
    "        images.append(Image.fromarray(screen))\n",
    "        o = obs\n",
    "        o = torch.tensor([o], dtype=torch.float32, device=device)\n",
    "        test_ep_len += 1\n",
    "    # print(\"begin writing image\")\n",
    "    now = datetime.now()\n",
    "    current_time = str(now.isoformat())\n",
    "    image_file = 'images/inverted-pendulum-v5-%s%d.gif'% (current_time.replace(\":\",\"-\"), epoch)\n",
    "    images[1].save(image_file, save_all=True, append_images=images[1:], loop=0, duration=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "obs, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "# o = list(obs['robot0_proprio-state']) + list(obs['object-state'])\n",
    "o =obs\n",
    "# env.viewer.set_camera(camera_id=0)\n",
    "\n",
    "\n",
    "# Define neutral value\n",
    "neutral = np.zeros(7)\n",
    "\n",
    "# Keep track of done variable to know when to break loop\n",
    "\n",
    "# Prepare for interaction with environment\n",
    "total_steps = params[\"steps_per_epoch\"] * params[\"epochs\"]\n",
    "start_time = time.time()\n",
    "\n",
    "o = torch.tensor([o], device=device)\n",
    "\n",
    "\n",
    "start_time_rec = datetime.now()\n",
    "r_true = 0\n",
    "total_main = 0\n",
    "ep_rew_main = 0\n",
    "reward_dict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ac.q\n",
    "print(\"Model_q's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "model = ac.pi\n",
    "print(\"Model_pi's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop: collect experience in env and update/log each epoch\n",
    "low = -1\n",
    "high = 1\n",
    "env.reset()\n",
    "for t in tqdm(range(total_steps)):\n",
    "    \n",
    "    # Until start_steps have elapsed, randomly sample actions\n",
    "    # from a uniform distribution for better exploration. Afterwards, \n",
    "    # use the learned policy (with some noise, via act_noise). \n",
    "    # env.render()\n",
    "    if t > params[\"start_steps\"]:\n",
    "        a = get_action(o, params[\"act_noise\"])      # Tensor\n",
    "        # print(\"a = \", a)\n",
    "        a_out = a.cpu().data.numpy()\n",
    "        a_cpu = a_out[0]\n",
    "    else:\n",
    "        a = torch.tensor([np.random.uniform(low, high)], dtype=torch.float32, device=device)\n",
    "        a_cpu = a.cpu().data.numpy()\n",
    "        # print(\"a = \", a)\n",
    "\n",
    "\n",
    "    # print(\"a_cpu = \", a_cpu[0])\n",
    "    # Step the env\n",
    "    obs2, r, d, _= env.step(a_cpu)\n",
    "    # print(\"env.step(a_cpu) = \", env.step(a_cpu))\n",
    "    # print(\"obs2 = \", obs2)\n",
    "    # env.render()\n",
    "\n",
    "    o2 = obs2\n",
    "\n",
    "\n",
    " \n",
    "    ep_len += 1\n",
    "    total_main += r\n",
    "    ep_ret += r\n",
    "\n",
    "    # print(\"a = \", a)\n",
    "    a_s = torch.tensor([a_cpu], dtype=torch.float32, device=device)\n",
    "    # print(\"a_s = \", a_s)\n",
    "    o2 = torch.tensor([o2], dtype=torch.float32, device=device)\n",
    "    r = torch.tensor([r], dtype=torch.float32, device=device)\n",
    "    d = torch.tensor([d], dtype=torch.float32, device=device)\n",
    "\n",
    "    # Store experience to replay buffer\n",
    "    memory.push(o, a_s, r, o2, d)\n",
    "    # print(\"o = \", o)\n",
    "    # print(\"a = \", a)\n",
    "    nep_log[\"train/o\"].log(o)\n",
    "    nep_log[\"train/a\"].log(a)\n",
    "    nep_log[\"train/r\"].log(r)\n",
    "    nep_log[\"train/o2\"].log(o2)\n",
    "    nep_log[\"train/d\"].log(d)\n",
    "\n",
    "    # Super critical, easy to overlook step: make sure to update \n",
    "    # most recent observation!\n",
    "    o=o2\n",
    "\n",
    "    \n",
    "    \n",
    "    # End of trajectory handling\n",
    "    if (ep_len == params[\"max_ep_len\"]):\n",
    "        ep_rew = ep_ret/params[\"max_ep_len\"]\n",
    "        ep_rew_main += ep_rew\n",
    "        obs, ep_ret, ep_len = env.reset(), 0, 0\n",
    "        # o = list(obs['robot0_proprio-state']) + list(obs['object-state'])\n",
    "        o = obs\n",
    "        o = torch.tensor([o], device=device)\n",
    "\n",
    "\n",
    "    # Update handling\n",
    "    if t >= params[\"update_after\"] and t % params[\"update_every\"] == 0:\n",
    "        for i in range(params[\"update_every\"]):\n",
    "\n",
    "            transitions = memory.sample(params[\"batch_size\"])\n",
    "            # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "            # detailed explanation). This converts batch-array of Transitions\n",
    "            # to Transition of batch-arrays.\n",
    "            batch = Transition(*zip(*transitions))\n",
    "            update(data=batch)\n",
    "\n",
    "    # End of epoch handling\n",
    "    if (t+1) % params[\"steps_per_epoch\"] == 0:\n",
    "        epoch = (t+1) // params[\"steps_per_epoch\"]\n",
    "        train_reward = ep_rew_main/(params[\"steps_per_epoch\"]/params[\"max_ep_len\"])\n",
    "        nep_log[\"train/reward\"].log(train_reward)\n",
    "        nep_log[\"train/total_main\"].log(total_main)\n",
    "        # print('train_rew_main = ', train_reward.cpu().data.numpy()[0])\n",
    "        print('train_rew_main = ', train_reward)\n",
    "        ep_rew_main = 0\n",
    "        # Test the performance of the deterministic version of the agent.\n",
    "        test_agent(epoch)\n",
    "        \n",
    "\n",
    "    # if (t+1) % params[\"steps_video\"] == 0:\n",
    "    #     epoch = (t+1) // params[\"steps_per_epoch\"]\n",
    "        # now = datetime.now()\n",
    "        # current_time = str(now.isoformat())\n",
    "        # print('current_time = ', current_time)\n",
    "        # video_agent(epoch)\n",
    "        # now = datetime.now()\n",
    "        # current_time = str(now.isoformat())\n",
    "        # print('current_time = ', current_time)\n",
    "\n",
    "    if (t+1) % params[\"save_model_len\"] == 0:\n",
    "        epoch = (t+1) // params[\"steps_per_epoch\"]\n",
    "        now = datetime.now()\n",
    "        current_time = str(now.isoformat())\n",
    "        torch.save({\n",
    "                    'model of ac.q': ac.q.state_dict(),\n",
    "                    'model of ac.pi': ac.pi.state_dict(),\n",
    "                    'q_optimizer_state_dict': q_optimizer.state_dict(),\n",
    "                    'pi_optimizer_state_dict': pi_optimizer.state_dict(),\n",
    "\n",
    "                    }, \"model_nn/model_nn_%s%d.pt\" % (current_time.replace(\":\",\"-\"), epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"pi_optimizer's state_dict:\")\n",
    "for var_name in pi_optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", pi_optimizer.state_dict()[var_name])\n",
    "\n",
    "print(\"q_optimizer's state_dict:\")\n",
    "for var_name in q_optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", q_optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "current_time = str(now.isoformat())\n",
    "\n",
    "\n",
    "\n",
    "torch.save({\n",
    "            'model of ac.q': ac.q.state_dict(),\n",
    "            'model of ac.pi': ac.pi.state_dict(),\n",
    "            'q_optimizer_state_dict': q_optimizer.state_dict(),\n",
    "            'pi_optimizer_state_dict': pi_optimizer.state_dict(),\n",
    "\n",
    "            }, \"model_nn/model_nn_%s%d.pt\" % (current_time.replace(\":\",\"-\"), epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nep_log.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a58f15cac8bdca84d481529dff1da9ea38dcd1e42654bb267244e94160251abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('RA_env_py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
